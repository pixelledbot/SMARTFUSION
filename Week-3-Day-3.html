<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Week 3 - Day 3 | SMARTFUSION</title>
  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
      background-color: #f9fafb;
      color: #1f2937;
      padding: 30px;
      max-width: 800px;
      margin: auto;
    }

    h1 {
      color: #2563eb;
      font-size: 2.2rem;
      margin-bottom: 5px;
    }

    h2 {
      font-size: 1.5rem;
      color: #1e40af;
      margin-top: 30px;
    }

    details {
      background: #ffffff;
      border-left: 5px solid #2563eb;
      padding: 15px 20px;
      margin-bottom: 15px;
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.04);
    }

    summary {
      font-size: 1.1rem;
      cursor: pointer;
      display: flex;
      justify-content: space-between;
      align-items: center;
      list-style: none;
    }

    summary::-webkit-details-marker {
      display: none;
    }

    summary::after {
      content: "+";
      font-weight: bold;
      color: #2563eb;
    }

    details[open] summary::after {
      content: "\2013";
    }

    details p, details ul li {
      font-size: 1rem;
      color: #374151;
      margin-top: 10px;
      margin-bottom: 8px;
    }

    a.back {
      display: inline-block;
      margin-top: 40px;
      text-decoration: none;
      color: #2563eb;
      font-weight: bold;
    }

    a.back:hover {
      text-decoration: underline;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      margin-top: 60px;
      color: #6b7280;
    }
  </style>
</head>
<body>

  <h1>Week 3 - Day 3</h1>
  <h2>Topics Covered</h2>

  <details>
    <summary>K-Nearest Neighbouring (KNN)</summary>
    <p><strong>Working:</strong> KNN is a simple, yet powerful supervised learning algorithm used for both classification and regression. It works by storing the entire training dataset and making predictions based on the majority label of the 'k' closest points in the feature space.</p>
    <p>When a new data point needs to be classified:
      <ul>
        <li>The distance (typically Euclidean) to all other training points is calculated.</li>
        <li>The k closest neighbors are selected based on this distance.</li>
        <li>The majority class among these neighbors is assigned to the new point (for classification).</li>
      </ul>
    </p>
    <p><strong>Example:</strong> Imagine we want to classify a fruit based on its weight and color. We have data on apples and oranges. When a new fruit comes in, KNN will find the closest fruits in the dataset and predict whether it’s more likely to be an apple or orange based on their majority.</p>
    <p><strong>Advantages:</strong> Easy to understand, no training phase required. <br>
       <strong>Disadvantages:</strong> Computationally expensive with large datasets, sensitive to irrelevant features and scale.</p>
  </details>

  <details>
    <summary>Confusion Matrix</summary>
    <p>A Confusion Matrix is a table used to evaluate the performance of a classification model. It shows the number of correct and incorrect predictions made by the classifier compared to the actual outcomes.</p>
    <p>For a binary classification task, it includes:
      <ul>
        <li><strong>True Positive (TP):</strong> Correctly predicted positive class</li>
        <li><strong>True Negative (TN):</strong> Correctly predicted negative class</li>
        <li><strong>False Positive (FP):</strong> Incorrectly predicted as positive</li>
        <li><strong>False Negative (FN):</strong> Incorrectly predicted as negative</li>
      </ul>
    </p>
    <p>From these values, metrics like Accuracy, Precision, Recall, and F1 Score can be calculated to get a more comprehensive understanding of the model’s performance.</p>
  </details>

  <a class="back" href="index.html">&larr; Back to Home</a>

  <footer>
    &copy; 2025 SMARTFUSION | Created by Simran Kaur
  </footer>

</body>
</html>
